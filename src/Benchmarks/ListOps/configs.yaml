### Global Configuration ###
global:
  benchmark_name: ListOps
  checkpoint_dir: src/Benchmarks/ListOps/checkpoints

  logging:
    info_freq: 100
    wandb:
      entity: m24842
      project: Machine Learning
  
  dataset: # 96,000 train samples
    name: ListOps
    splits:
      train:
        root: data
        split: train
        min_len: 1024
        max_len: 1024
        warmup_epochs: 0
      val:
        root: data
        split: val
        max_len: 2048
      test:
        root: data
        split: test
        max_len: 2048
  
  checkpoint_freq: 500

### Defaults ###
general_defaults: &general_defaults
  load_checkpoint: False
  seed: 0
  batch_size: 64
  epochs: 10
  grad_clip_norm: 1.0
  mixed_precision: False

optimizer_defaults: &optimizer_defaults
  name: AdamW
  lr: 3e-4
  weight_decay: 0.01

scheduler_defaults: &scheduler_defaults
  name: CosineAnnealingLR
  T_max: 15000 # ceil(train_samples / batch_size) * epochs
  eta_min: 1e-6

Transformer_defaults: &Transformer_defaults
  general: &Transformer_general
    <<: *general_defaults
  
  model: &Transformer_model
    name: Transformer
    emb_dim: 128
    ff_dim: 128
    n_heads: 4
    n_layers: 4
    input_dim: 17
    output_dim: 10
    attn_sink: False
    dropout: 0.0
    causal: False
    use_embedding: True
    pos_encoding: rope
  
  optimizer:
    <<: *optimizer_defaults
  
  scheduler:
    <<: *scheduler_defaults

CompressionTransformer_defaults: &CompressionTransformer_defaults
  general: &CompressionTransformer_general
    <<: *general_defaults
  
  model: &CompressionTransformer_model
    name: CompressionTransformer
    emb_dim: 128
    ff_dim: 128
    mem_dim: 16
    n_heads: 4
    n_layers: 4
    input_dim: 17
    output_dim: 10
    attn_sink: False
    dropout: 0.0
    causal: False
    use_embedding: True
    pos_encoding: rope
  
  optimizer:
    <<: *optimizer_defaults
  
  scheduler:
    <<: *scheduler_defaults

LinearTransformer_defaults: &LinearTransformer_defaults
  general: &LinearTransformer_general
    <<: *general_defaults
  
  model: &LinearTransformer_model
    name: LinearTransformer
    emb_dim: 128
    ff_dim: 128
    n_heads: 4
    n_layers: 4
    input_dim: 17
    output_dim: 10
    attn_sink: False
    dropout: 0.0
    causal: False
    use_embedding: True
    pos_encoding: rope
  
  optimizer:
    <<: *optimizer_defaults
  
  scheduler:
    <<: *scheduler_defaults

OrthoLinearTransformer_defaults: &OrthoLinearTransformer_defaults
  general: &OrthoLinearTransformer_general
    <<: *general_defaults
  
  model: &OrthoLinearTransformer_model
    name: OrthoLinearTransformer
    emb_dim: 128
    ff_dim: 128
    n_heads: 4
    n_layers: 4
    input_dim: 17
    output_dim: 10
    attn_sink: False
    dropout: 0.0
    causal: False
    use_embedding: True
    pos_encoding: rope
  
  optimizer:
    <<: *optimizer_defaults
  
  scheduler:
    <<: *scheduler_defaults

SlidingWindowTransformer_defaults: &SlidingWindowTransformer_defaults
  general: &SlidingWindowTransformer_general
    <<: *general_defaults
  
  model: &SlidingWindowTransformer_model
    name: SlidingWindowTransformer
    emb_dim: 128
    ff_dim: 128
    window_len: 64
    use_flex_attn: False
    dilate: True
    # dilation_factor: 64
    n_heads: 4
    n_layers: 4
    input_dim: 17
    output_dim: 10
    attn_sink: False
    dropout: 0.0
    causal: False
    use_embedding: True
    pos_encoding: rope
  
  optimizer:
    <<: *optimizer_defaults
  
  scheduler:
    <<: *scheduler_defaults

Mamba2_defaults: &Mamba2_defaults
  general: &Mamba2_general
    <<: *general_defaults
  
  model: &Mamba2_model
    name: Mamba2
    emb_dim: 128
    n_heads: 4
    n_layers: 8
    input_dim: 17
    output_dim: 10
    bidirectional: True
    use_embedding: True
    chunk_size: 64
  
  optimizer:
    <<: *optimizer_defaults
  
  scheduler:
    <<: *scheduler_defaults

SeqMLP_defaults: &SeqMLP_defaults
  general: &SeqMLP_general
    <<: *general_defaults
  
  model: &SeqMLP_model
    name: SeqMLP
    emb_dim: 128
    n_heads: 4
    n_layers: 8
    input_dim: 17
    output_dim: 10
    mode: bidirectional
    use_embedding: True
  
  optimizer:
    <<: *optimizer_defaults
  
  scheduler:
    <<: *scheduler_defaults

### Experiments Configuration ###
experiments:
  - <<: *SeqMLP_defaults
    general:
      <<: *SeqMLP_general
      seed: 0

  - <<: *SeqMLP_defaults
    general:
      <<: *SeqMLP_general
      seed: 42
  
  - <<: *SeqMLP_defaults
    general:
      <<: *SeqMLP_general
      seed: 1111
  
  - <<: *SeqMLP_defaults
    general:
      <<: *SeqMLP_general
      seed: 2222
  
  - <<: *SeqMLP_defaults
    general:
      <<: *SeqMLP_general
      seed: 3333

  - <<: *Mamba2_defaults
    general:
      <<: *Mamba2_general
      seed: 0

  - <<: *Mamba2_defaults
    general:
      <<: *Mamba2_general
      seed: 42
  
  - <<: *Mamba2_defaults
    general:
      <<: *Mamba2_general
      seed: 1111
  
  - <<: *Mamba2_defaults
    general:
      <<: *Mamba2_general
      seed: 2222
  
  - <<: *Mamba2_defaults
    general:
      <<: *Mamba2_general
      seed: 3333
  
  - <<: *SlidingWindowTransformer_defaults
    general:
      <<: *SlidingWindowTransformer_general
      seed: 0

  - <<: *SlidingWindowTransformer_defaults
    general:
      <<: *SlidingWindowTransformer_general
      seed: 42
  
  - <<: *SlidingWindowTransformer_defaults
    general:
      <<: *SlidingWindowTransformer_general
      seed: 1111
  
  - <<: *SlidingWindowTransformer_defaults
    general:
      <<: *SlidingWindowTransformer_general
      seed: 2222
  
  - <<: *SlidingWindowTransformer_defaults
    general:
      <<: *SlidingWindowTransformer_general
      seed: 3333
  
  - <<: *CompressionTransformer_defaults
    general:
      <<: *CompressionTransformer_general
      seed: 0
  
  - <<: *CompressionTransformer_defaults
    general:
      <<: *CompressionTransformer_general
      seed: 42
  
  - <<: *CompressionTransformer_defaults
    general:
      <<: *CompressionTransformer_general
      seed: 1111
  
  - <<: *CompressionTransformer_defaults
    general:
      <<: *CompressionTransformer_general
      seed: 2222
  
  - <<: *CompressionTransformer_defaults
    general:
      <<: *CompressionTransformer_general
      seed: 3333
  
  - <<: *Transformer_defaults
    general:
      <<: *Transformer_general
      seed: 0
  
  - <<: *Transformer_defaults
    general:
      <<: *Transformer_general
      seed: 42
  
  - <<: *Transformer_defaults
    general:
      <<: *Transformer_general
      seed: 1111
  
  - <<: *Transformer_defaults
    general:
      <<: *Transformer_general
      seed: 2222
  
  - <<: *Transformer_defaults
    general:
      <<: *Transformer_general
      seed: 3333
  
  - <<: *OrthoLinearTransformer_defaults
    general:
      <<: *OrthoLinearTransformer_general
      seed: 0
  
  - <<: *OrthoLinearTransformer_defaults
    general:
      <<: *OrthoLinearTransformer_general
      seed: 42
  
  - <<: *OrthoLinearTransformer_defaults
    general:
      <<: *OrthoLinearTransformer_general
      seed: 1111
  
  - <<: *OrthoLinearTransformer_defaults
    general:
      <<: *OrthoLinearTransformer_general
      seed: 2222
  
  - <<: *OrthoLinearTransformer_defaults
    general:
      <<: *OrthoLinearTransformer_general
      seed: 3333
  
  - <<: *LinearTransformer_defaults
    general:
      <<: *LinearTransformer_general
      seed: 0
  
  - <<: *LinearTransformer_defaults
    general:
      <<: *LinearTransformer_general
      seed: 42
  
  - <<: *LinearTransformer_defaults
    general:
      <<: *LinearTransformer_general
      seed: 1111
  
  - <<: *LinearTransformer_defaults
    general:
      <<: *LinearTransformer_general
      seed: 2222
  
  - <<: *LinearTransformer_defaults
    general:
      <<: *LinearTransformer_general
      seed: 3333
  