### Global Configuration ###
global:
  benchmark_name: Tiny Shakespeare
  checkpoint_dir: src/Benchmarks/TinyShakespeare/checkpoints

  logging:
    info_freq: 100
    wandb:
      entity: m24842
      project: Machine Learning
      metrics: [acc, loss, ppl, seq_len, lr]
  
  dataset: # 304,141 train tokens
    name: TinyShakespeare
    splits:
      train:
        train: True
        tokenizer: EleutherAI/gpt-neox-20b
        min_len: 256
        max_len: 256
        warmup_epochs: 0
      test:
        train: False
        tokenizer: EleutherAI/gpt-neox-20b
        max_len: 256
  
  checkpoint_freq: 500

### Defaults ###
general_defaults: &general_defaults
  load_checkpoint: False
  seed: 0
  batch_size: 16
  accumulation_steps: 0
  train_steps: 10000
  grad_clip_norm: 1.0
  mixed_precision: True

optimizer_defaults: &optimizer_defaults
  name: AdamW
  lr: 6e-4
  weight_decay: 0.1
  betas: [0.9, 0.95]

scheduler_defaults: &scheduler_defaults
  name: CosineAnnealingLRWithWarmup
  eta_min: 1e-5
  num_warmup_steps: 0
  num_training_steps: 10000

Transformer_defaults: &Transformer_defaults
  general: &Transformer_general
    <<: *general_defaults
  
  model: &Transformer_model
    name: Transformer
    emb_dim: 128
    mlp_dim: 128
    n_heads: 4
    n_layers: 6
    input_dim: 50277
    output_dim: 50277
    attn_sink: False
    mlp_bias: False
    attn_bias: False
    dropout: 0.0
    causal: True
    use_embedding: True
    pos_encoding: xpos
  
  optimizer:
    <<: *optimizer_defaults
  
  scheduler:
    <<: *scheduler_defaults

CompressionTransformer_defaults: &CompressionTransformer_defaults
  general: &CompressionTransformer_general
    <<: *general_defaults
  
  model: &CompressionTransformer_model
    name: CompressionTransformer
    emb_dim: 128
    mlp_dim: 128
    mem_dim: 16
    n_heads: 4
    n_layers: 6
    input_dim: 50277
    output_dim: 50277
    attn_sink: False
    mlp_bias: False
    attn_bias: False
    dropout: 0.0
    causal: True
    use_embedding: True
    pos_encoding: xpos
  
  optimizer:
    <<: *optimizer_defaults
  
  scheduler:
    <<: *scheduler_defaults

LinearTransformer_defaults: &LinearTransformer_defaults
  general: &LinearTransformer_general
    <<: *general_defaults
  
  model: &LinearTransformer_model
    name: LinearTransformer
    emb_dim: 128
    mlp_dim: 128
    n_heads: 4
    n_layers: 6
    input_dim: 50277
    output_dim: 50277
    attn_sink: False
    mlp_bias: False
    attn_bias: False
    dropout: 0.0
    causal: True
    use_embedding: True
    pos_encoding: xpos
  
  optimizer:
    <<: *optimizer_defaults
  
  scheduler:
    <<: *scheduler_defaults

OrthoLinearTransformer_defaults: &OrthoLinearTransformer_defaults
  general: &OrthoLinearTransformer_general
    <<: *general_defaults
  
  model: &OrthoLinearTransformer_model
    name: OrthoLinearTransformer
    emb_dim: 128
    mlp_dim: 128
    n_heads: 4
    n_layers: 6
    input_dim: 50277
    output_dim: 50277
    attn_sink: False
    mlp_bias: False
    attn_bias: False
    dropout: 0.0
    causal: True
    use_embedding: True
    pos_encoding: xpos
  
  optimizer:
    <<: *optimizer_defaults
  
  scheduler:
    <<: *scheduler_defaults

SlidingWindowTransformer_defaults: &SlidingWindowTransformer_defaults
  general: &SlidingWindowTransformer_general
    <<: *general_defaults
  
  model: &SlidingWindowTransformer_model
    name: SlidingWindowTransformer
    emb_dim: 128
    mlp_dim: 128
    window_len: 64
    masked_window: False
    dilate: True
    # dilation_factor: 64
    n_heads: 4
    n_layers: 6
    input_dim: 50277
    output_dim: 50277
    attn_sink: False
    mlp_bias: False
    attn_bias: False
    dropout: 0.0
    causal: True
    use_embedding: True
    pos_encoding: xpos
  
  optimizer:
    <<: *optimizer_defaults
  
  scheduler:
    <<: *scheduler_defaults

FastTransformer_defaults: &FastTransformer_defaults
  general: &FastTransformer_general
    <<: *general_defaults
  
  model: &FastTransformer_model
    name: FastTransformer
    emb_dim: 128
    mlp_dim: 128
    window_len: 64
    masked_window: True
    n_dilations: 2
    # dilation_factor: 64
    n_heads: 4
    n_layers: 6
    input_dim: 50277
    output_dim: 50277
    attn_sink: False
    mlp_bias: False
    attn_bias: False
    dropout: 0.0
    causal: True
    use_embedding: True
    pos_encoding: xpos
  
  optimizer:
    <<: *optimizer_defaults
  
  scheduler:
    <<: *scheduler_defaults

DiffusionTransformer_defaults: &DiffusionTransformer_defaults
  general: &DiffusionTransformer_general
    <<: *general_defaults
    use_data_fn: 0
    batch_size: 16
    accumulation_steps: 7
    train_steps: 1000000
  
  model: &DiffusionTransformer_model
    name: DiffusionTransformer
    emb_dim: 128
    mlp_dim: 128
    n_heads: 4
    n_layers: 6
    input_dim: 50277
    output_dim: 50277
    attn_sink: False
    mlp_bias: False
    attn_bias: False
    dropout: 0.0
    pos_encoding: rope
  
  optimizer:
    <<: *optimizer_defaults
    lr: 1e-4
  
  scheduler:
    <<: *scheduler_defaults
    num_warmup_steps: 1000
    num_training_steps: 1000000
    eta_min: 1e-6

### Experiments Configuration ###
experiments:
  - <<: *DiffusionTransformer_defaults
    general:
      <<: *DiffusionTransformer_general
      seed: 0

  # - <<: *DiffusionTransformer_defaults
  #   general:
  #     <<: *DiffusionTransformer_general
  #     seed: 42
  
  # - <<: *DiffusionTransformer_defaults
  #   general:
  #     <<: *DiffusionTransformer_general
  #     seed: 1111
  
  # - <<: *DiffusionTransformer_defaults
  #   general:
  #     <<: *DiffusionTransformer_general
  #     seed: 2222
  
  # - <<: *DiffusionTransformer_defaults
  #   general:
  #     <<: *DiffusionTransformer_general
  #     seed: 3333
  
  # - <<: *FastTransformer_defaults
  #   general:
  #     <<: *FastTransformer_general
  #     seed: 0

  # - <<: *FastTransformer_defaults
  #   general:
  #     <<: *FastTransformer_general
  #     seed: 42
  
  # - <<: *FastTransformer_defaults
  #   general:
  #     <<: *FastTransformer_general
  #     seed: 1111
  
  # - <<: *FastTransformer_defaults
  #   general:
  #     <<: *FastTransformer_general
  #     seed: 2222
  
  # - <<: *FastTransformer_defaults
  #   general:
  #     <<: *FastTransformer_general
  #     seed: 3333
  
  # - <<: *SlidingWindowTransformer_defaults
  #   general:
  #     <<: *SlidingWindowTransformer_general
  #     seed: 0

  # - <<: *SlidingWindowTransformer_defaults
  #   general:
  #     <<: *SlidingWindowTransformer_general
  #     seed: 42
  
  # - <<: *SlidingWindowTransformer_defaults
  #   general:
  #     <<: *SlidingWindowTransformer_general
  #     seed: 1111
  
  # - <<: *SlidingWindowTransformer_defaults
  #   general:
  #     <<: *SlidingWindowTransformer_general
  #     seed: 2222
  
  # - <<: *SlidingWindowTransformer_defaults
  #   general:
  #     <<: *SlidingWindowTransformer_general
  #     seed: 3333
  
  # - <<: *CompressionTransformer_defaults
  #   general:
  #     <<: *CompressionTransformer_general
  #     seed: 0
  
  # - <<: *CompressionTransformer_defaults
  #   general:
  #     <<: *CompressionTransformer_general
  #     seed: 42
  
  # - <<: *CompressionTransformer_defaults
  #   general:
  #     <<: *CompressionTransformer_general
  #     seed: 1111
  
  # - <<: *CompressionTransformer_defaults
  #   general:
  #     <<: *CompressionTransformer_general
  #     seed: 2222
  
  # - <<: *CompressionTransformer_defaults
  #   general:
  #     <<: *CompressionTransformer_general
  #     seed: 3333
  
  # - <<: *Transformer_defaults
  #   general:
  #     <<: *Transformer_general
  #     seed: 0
  
  # - <<: *Transformer_defaults
  #   general:
  #     <<: *Transformer_general
  #     seed: 42
  
  # - <<: *Transformer_defaults
  #   general:
  #     <<: *Transformer_general
  #     seed: 1111
  
  # - <<: *Transformer_defaults
  #   general:
  #     <<: *Transformer_general
  #     seed: 2222
  
  # - <<: *Transformer_defaults
  #   general:
  #     <<: *Transformer_general
  #     seed: 3333
  
  # - <<: *OrthoLinearTransformer_defaults
  #   general:
  #     <<: *OrthoLinearTransformer_general
  #     seed: 0
  
  # - <<: *OrthoLinearTransformer_defaults
  #   general:
  #     <<: *OrthoLinearTransformer_general
  #     seed: 42
  
  # - <<: *OrthoLinearTransformer_defaults
  #   general:
  #     <<: *OrthoLinearTransformer_general
  #     seed: 1111
  
  # - <<: *OrthoLinearTransformer_defaults
  #   general:
  #     <<: *OrthoLinearTransformer_general
  #     seed: 2222
  
  # - <<: *OrthoLinearTransformer_defaults
  #   general:
  #     <<: *OrthoLinearTransformer_general
  #     seed: 3333
  
  # - <<: *LinearTransformer_defaults
  #   general:
  #     <<: *LinearTransformer_general
  #     seed: 0
  
  # - <<: *LinearTransformer_defaults
  #   general:
  #     <<: *LinearTransformer_general
  #     seed: 42
  
  # - <<: *LinearTransformer_defaults
  #   general:
  #     <<: *LinearTransformer_general
  #     seed: 1111
  
  # - <<: *LinearTransformer_defaults
  #   general:
  #     <<: *LinearTransformer_general
  #     seed: 2222
  
  # - <<: *LinearTransformer_defaults
  #   general:
  #     <<: *LinearTransformer_general
  #     seed: 3333
  