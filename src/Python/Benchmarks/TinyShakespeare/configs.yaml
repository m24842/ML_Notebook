global:
  benchmark_name: Tiny Shakespeare
  output_dir: src/Python/Benchmarks/TinyShakespeare/models

  logging:
    info_freq: 100
    local_log_path: src/Python/Benchmarks/TinyShakespeare/experiments.log
    wandb:
      entity: m24842
      project: Machine Learning
      metrics: [acc, loss, seq_len]
  
  dataset:
    name: TinyShakespeare
    splits:
      train:
        train: True
        min_len: 1024
        max_len: 2048
        warmup_epochs: 3
      test:
        train: False
        max_len: 2048
  
  checkpoint_freq: 500

experiments:
  - general:
      model_name: CompressionTransformer
      load_checkpoint: False
      seed: 0
      batch_size: 16
      epochs: 10
      grad_clip_norm: 1.0
    
    model:
      emb_dim: 128
      mlp_dim: 128
      mem_dim: 16
      n_heads: 4
      n_layers: 4
      input_dim: 28996
      output_dim: 28996
      dropout: 0.1
      causal: True
      use_embedding: True
    
    optimizer:
      name: AdamW
      lr: 3e-4
      weight_decay: 0.01
    
    scheduler:
      name: CosineAnnealingLR
      T_max: 10
      eta_min: 1e-6